import numpy as np
from sklearn.tree import DecisionTreeClassifier


def make_data(m=100, n=3, balanced=True, seed=None):
    """make binary data for a binary Decision Tree Classifier"""
    #random seed
    if seed is True:
        seed = __import__("random").randint(0, 1000)
        print("seed =", seed)
    if seed:
        np.random.seed(int(seed))
    
    #make data
    pp = np.random.random(size=n)  # proportions of True in each feature
    X = np.random.binomial(n=1, p=pp, size=(m,n))
    Xoriginal = X.copy().astype('float')  # Xoriginal will be returned; mangle the X
    
    #reverse the boolean values of some features (e.g. is_old >> is_new)
    def func(X):
        mask = np.random.binomial(n=1, p=0.25, size=n).astype("uint8")
        ff = np.array([np.vectorize(lambda x:x), np.logical_not])[mask]
        Xbool = X.astype("bool")
        Xnew = np.empty_like(X, dtype='bool')
        for i,row in enumerate(Xbool):
            Xnew[i] = [f(x) for f,x in zip(ff,row)]  
        return Xnew.astype('uint8')
    
    X = func(X)
    
    #f!ck up a certain proportion of each feature by reversing the boolean value in a coresponding cell
    high = (1/n)**3  # maximum allowed proportion to be mangled (the lower n the higher this proportion)
    pp = np.random.uniform(low=0, high=high, size=n)  # proportion of each feature to be mangled
    g = (np.random.permutation(m)[:int(m*p)] for p in pp)
    for j,nx in enumerate(g):
        X[nx,j] = np.logical_not(X[nx,j])  # reverse the boolean feature in certain cells
        
    #compute the target
    y = X.sum(axis=1)
    
    #balanced or imbalanced dataset?
    lower = abs(float(balanced)) if isinstance(balanced,float) else 0.5
    threshold = 0.5 if balanced is True else np.random.uniform(lower, 0.99)
    q = np.quantile(y, q=threshold)
    y = (y >= q).astype('uint8')
    return(Xoriginal,y)

#######################################################################

X,y = make_data(m=200, n=5, balanced=True, seed=285)

from sklearn.model_selection import train_test_split
Xtrain,Xtest, ytrain,ytest = train_test_split(X,y)
(m,n) = Xtrain.shape

from sklearn.tree import DecisionTreeClassifier
md = DecisionTreeClassifier(max_depth=None, min_samples_split=max(m//100, 6), min_samples_leaf=max(m//200,3))
#md = DecisionTreeClassifier(max_depth=None, min_samples_split=6, min_samples_leaf=5)
md.fit(Xtrain, ytrain)
acc = md.score(Xtest, ytest)
print("accuracy", acc)
print(md.feature_importances_.round(3).tolist())


###

def gini(*_):
    a,b = _ if(len(_)==2) else _[0] if(len(_)==1) else (None,None)
    total = sum((a,b))
    g = 1 - (a/total)**2 - (b/total)**2
    return(g)  #g = gini-impurity-measure

def Gini(*_):
    a,b,c,d = _ if(len(_)==4) else _[0] if(len(_)==1) else (None,)*4
    g1 = gini(a,b)
    g0 = gini(c,d)
    n1 = sum([a,b])
    n0 = sum([c,d])
    N = n1+n0
    G = (n1/N)*g1 + (n0/N)*g0
    return(G)
    


n = X.shape[1]
l = list()
for j in range(n):
    t = tuple(np.c_[X[:,j], y].astype("uint8").tolist())
    values = ([1,1],[1,0],[0,1],[0,0])
    counts = [t.count(v) for v in values]
    G = Gini(counts)
    print(j,G)
    l.append((j,G))

bestGini = min(l, key=lambda t:t[-1])
bestRootNode = min(l, key=lambda t:t[-1])[0]

################################################################
################################################################
################################################################


"""
Create a working tree by a random process
"""

class Node():
    def __init__(self, rule:'feature number to check if true'=None):
        self.rule = rule
        self.next_node_true = None
        self.next_node_false = None
    
    def forward(self, x):
        j = self.rule

        if int(x[j]) == 1:
            ans = self.next_node_true.forward(x)
        elif int(x[j]) == 0:
            ans = self.next_node_false.forward(x)
        else: raise ValueError("bad value" + str(x[j]))
        return ans
    
    def predict(self,x):
        return self.forward(x)
    
    def __call__(self, x):
        return self.forward(x)
    
    
class Leaf():
    def __init__(self, predicted_class=None):
        self.predicted_class = predicted_class
        
    def forward(self, x):
        ans = self.predicted_class
        return ans


def build_tree(depth):
    global classes, n
    global l, leaves_counter, nodes_counter
    #base case
    if depth==1:
        random_class = classes[randint(0, len(classes)-1)]
        leaf = Leaf(predicted_class=random_class)
        leaves_counter += 1
        print("attaching leaf", leaves_counter, "at level", depth)
        l.append(leaf)
        return leaf
    #non-base case
    random_rule = randint(0, n-1)
    new = Node(rule=random_rule)
    nodes_counter += 1
    print("created node", nodes_counter, "at level", depth)
    
    node1 = build_tree(depth-1) if random()<0.8 else Leaf(predicted_class=classes[randint(0, len(classes)-1)])
    node0 = build_tree(depth-1) if random()<0.8 else Leaf(predicted_class=classes[randint(0, len(classes)-1)])
    new.next_node_true = node1
    new.next_node_false = node0
    return(new)

#############################################################################   
    

leaf1 = Leaf(predicted_class=1)
leaf0 = Leaf(predicted_class=0)

root = Node(rule=0)
nodeA = Node(rule=1)
nodeB = Node(rule=2)

root.next_node_true = nodeA
root.next_node_false = nodeB  

nodeA.next_node_true = leaf0
nodeA.next_node_false = leaf1

nodeB.next_node_true = leaf1
nodeB.next_node_false = leaf0



# use a recursive function (with some random behaviour inside) to create a tree
from random import randint, random

(m,n) = 100,5
classes = (0,1,2,3)
l = list()
leaves_counter = 0
nodes_counter = 0


tree = build_tree(depth=4)

xi = [randint(0,1) for _ in range(n)]

ypred = tree(xi)
ypred = tree.predict(xi)
print(ypred)


##test the tree
import numpy as np
X = np.random.randint(0,2, size=(m,n))
ypred = [tree.predict(x) for x in X]
print(ypred)
