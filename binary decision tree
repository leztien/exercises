import numpy as np
from sklearn.tree import DecisionTreeClassifier


def make_data(m=100, n=3, balanced=True, seed=None):
    """make binary data for a binary Decision Tree Classifier"""
    #random seed
    if seed is True:
        seed = __import__("random").randint(0, 1000)
        print("seed =", seed)
    if seed:
        np.random.seed(int(seed))
    
    #make data
    pp = np.random.random(size=n)  # proportions of True in each feature
    X = np.random.binomial(n=1, p=pp, size=(m,n))
    Xoriginal = X.copy().astype('float')  # Xoriginal will be returned; mangle the X
    
    #reverse the boolean values of some features (e.g. is_old >> is_new)
    def func(X):
        mask = np.random.binomial(n=1, p=0.25, size=n).astype("uint8")
        ff = np.array([np.vectorize(lambda x:x), np.logical_not])[mask]
        Xbool = X.astype("bool")
        Xnew = np.empty_like(X, dtype='bool')
        for i,row in enumerate(Xbool):
            Xnew[i] = [f(x) for f,x in zip(ff,row)]  
        return Xnew.astype('uint8')
    
    X = func(X)
    
    #f!ck up a certain proportion of each feature by reversing the boolean value in a coresponding cell
    high = (1/n)**3  # maximum allowed proportion to be mangled (the lower n the higher this proportion)
    pp = np.random.uniform(low=0, high=high, size=n)  # proportion of each feature to be mangled
    g = (np.random.permutation(m)[:int(m*p)] for p in pp)
    for j,nx in enumerate(g):
        X[nx,j] = np.logical_not(X[nx,j])  # reverse the boolean feature in certain cells
        
    #compute the target
    y = X.sum(axis=1)
    
    #balanced or imbalanced dataset?
    lower = abs(float(balanced)) if isinstance(balanced,float) else 0.5
    threshold = 0.5 if balanced is True else np.random.uniform(lower, 0.99)
    q = np.quantile(y, q=threshold)
    y = (y >= q).astype('uint8')
    return(Xoriginal,y)

#######################################################################

X,y = make_data(m=200, n=5, balanced=True, seed=285)

from sklearn.model_selection import train_test_split
Xtrain,Xtest, ytrain,ytest = train_test_split(X,y)
(m,n) = Xtrain.shape

from sklearn.tree import DecisionTreeClassifier
md = DecisionTreeClassifier(max_depth=None, min_samples_split=max(m//100, 6), min_samples_leaf=max(m//200,3))
#md = DecisionTreeClassifier(max_depth=None, min_samples_split=6, min_samples_leaf=5)
md.fit(Xtrain, ytrain)
acc = md.score(Xtest, ytest)
print("accuracy", acc)
print(md.feature_importances_.round(3).tolist())


###

def gini(*_):
    a,b = _ if(len(_)==2) else _[0] if(len(_)==1) else (None,None)
    total = sum((a,b))
    g = 1 - (a/total)**2 - (b/total)**2
    return(g)  #g = gini-impurity-measure

def Gini(*_):
    a,b,c,d = _ if(len(_)==4) else _[0] if(len(_)==1) else (None,)*4
    g1 = gini(a,b)
    g0 = gini(c,d)
    n1 = sum([a,b])
    n0 = sum([c,d])
    N = n1+n0
    G = (n1/N)*g1 + (n0/N)*g0
    return(G)
    


n = X.shape[1]
l = list()
for j in range(n):
    t = tuple(np.c_[X[:,j], y].astype("uint8").tolist())
    values = ([1,1],[1,0],[0,1],[0,0])
    counts = [t.count(v) for v in values]
    G = Gini(counts)
    print(j,G)
    l.append((j,G))

bestGini = min(l, key=lambda t:t[-1])
bestRootNode = min(l, key=lambda t:t[-1])[0]

