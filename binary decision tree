
"""
Create a "working" Decision Tree (by a random process within a recursive function)
"""

class Node():
    def __init__(self, rule:'feature number to check if true'=None):
        self.rule = rule
        self.next_node_true = None
        self.next_node_false = None
    def forward(self, x):
        j = self.rule
        if int(x[j]) == 1:
            ans = self.next_node_true.forward(x)
        elif int(x[j]) == 0:
            ans = self.next_node_false.forward(x)
        else: raise ValueError("bad value" + str(x[j]))
        return ans
    def predict(self,x):
        return self.forward(x)
    def __call__(self, x):
        return self.forward(x)
    
    
class Leaf():
    def __init__(self, predicted_class=None):
        self.predicted_class = self.rule = predicted_class  
    def forward(self, x):
        ans = self.predicted_class
        return ans


from random import randint, random
class Tree:
    def __init__(self, depth, n_features=3, n_classes=2):
        self.depth = int(depth)
        self.n_features = n_features
        self.n_classes = n_classes
        self.graph = ["digraph Tree {node [shape=box];",]
        self._nodes_counter = 0
        self.root = self._grow_tree(self.depth)
        self.graph.append("}")
        self.graph = str.join("\n", self.graph)


    def _grow_tree(self, current_depth):
        def add_leaf(self):  # adds a Leaf in case of a base-case or due to the probability (see further in the code)
            r = randint(0, self.n_classes-1)   # random class
            leaf = Leaf(predicted_class=r)
            
            #graphviz
            leaf._node_number = self._nodes_counter  #for graphviz
            self.graph.append('{} [color="black", fillcolor="green", style="filled", label="class {}"]'.format(leaf._node_number, leaf.rule))
            
            self._nodes_counter += 1
            print(leaf._node_number, "attached leaf node", self._nodes_counter-1, "at level", self.depth-current_depth+1)
            return(leaf)
        
        #base case
        if current_depth==1:
            return(add_leaf(self))  # returns a Leaf()
            
        #non-base case
        r = randint(0, self.n_features-1)   # random feature to separate on
        new = Node(rule=r)
        new._node_number = self._nodes_counter  # for graphviz
        self._nodes_counter += 1
        print(new._node_number, "created node", self._nodes_counter-1, "at level", self.depth-current_depth+1)
        
        #probabilities for ending in a leaf
        threashold = 1/self.depth * (self.depth - current_depth)  # linear probability growth
        p1,p0 = random(), random()  # if this probability is lower than the threshold then a Leaf is created (instead of a Node)
        
        node1 = self._grow_tree(current_depth-1) if p1>threashold else add_leaf(self)
        node0 = self._grow_tree(current_depth-1) if p0>threashold else add_leaf(self)
        new.next_node_true = node1   #node1 & node0 are either a Node or a Leaf
        new.next_node_false = node0
        
        #graphviz
        self.graph.append('{} -> {} [labeldistance=2.5, labelangle=45, fontsize=11, headlabel="True"]'.format(new._node_number, new.next_node_true._node_number))
        self.graph.append('{} -> {} [labeldistance=2.5, labelangle=-45, fontsize=11, headlabel="False"]'.format(new._node_number, new.next_node_false._node_number))
        self.graph.append('{} [color="black", fillcolor="orange", style="filled", label="x{} = True"]'.format(new._node_number, new.rule))
        return(new)  # returns a new Node
    
    
    def predict(self, X):
        if not hasattr(X[0], "__len__"):
            ypred = self.root.forward(X)
        else:
            ypred = [self.root.forward(x) for x in X]
        return ypred

#############################################################################   

(m,n) = 100, 10
k =3
tree = Tree(depth=8, n_features=n, n_classes=k)

#graphviz
string = tree.graph
try: from graphviz import Source
except: print("Unable to import graphviz")
else: 
    graph = Source(string, filename="image", format="png")
    graph.view()


#test
import numpy as np
X = np.random.randint(0,2, size=(m,n))
x = X[0]

ypred = tree.predict(x)
print(ypred)

ypred = tree.predict(X)
print(ypred)


##############################################################################
##############################################################################
##############################################################################

import numpy as np
from sklearn.tree import DecisionTreeClassifier


def make_data(m=100, n=3, balanced=True, seed=None):
    """make binary data for a binary Decision Tree Classifier"""
    #random seed
    if seed is True:
        seed = __import__("random").randint(0, 1000)
        print("seed =", seed)
    if seed:
        np.random.seed(int(seed))
    
    #make data
    pp = np.random.random(size=n)  # proportions of True in each feature
    X = np.random.binomial(n=1, p=pp, size=(m,n))
    Xoriginal = X.copy().astype('float')  # Xoriginal will be returned; mangle the X
    
    #reverse the boolean values of some features (e.g. is_old >> is_new)
    def func(X):
        mask = np.random.binomial(n=1, p=0.25, size=n).astype("uint8")
        ff = np.array([np.vectorize(lambda x:x), np.logical_not])[mask]
        Xbool = X.astype("bool")
        Xnew = np.empty_like(X, dtype='bool')
        for i,row in enumerate(Xbool):
            Xnew[i] = [f(x) for f,x in zip(ff,row)]  
        return Xnew.astype('uint8')
    
    X = func(X)
    
    #f!ck up a certain proportion of each feature by reversing the boolean value in a coresponding cell
    high = (1/n)**3  # maximum allowed proportion to be mangled (the lower n the higher this proportion)
    pp = np.random.uniform(low=0, high=high, size=n)  # proportion of each feature to be mangled
    g = (np.random.permutation(m)[:int(m*p)] for p in pp)
    for j,nx in enumerate(g):
        X[nx,j] = np.logical_not(X[nx,j])  # reverse the boolean feature in certain cells
        
    #compute the target
    y = X.sum(axis=1)
    
    #balanced or imbalanced dataset?
    lower = abs(float(balanced)) if isinstance(balanced,float) else 0.5
    threshold = 0.5 if balanced is True else np.random.uniform(lower, 0.99)
    q = np.quantile(y, q=threshold)
    y = (y >= q).astype('uint8')
    return(Xoriginal,y)

#######################################################################

X,y = make_data(m=200, n=5, balanced=True, seed=285)

from sklearn.model_selection import train_test_split
Xtrain,Xtest, ytrain,ytest = train_test_split(X,y)
(m,n) = Xtrain.shape

from sklearn.tree import DecisionTreeClassifier
md = DecisionTreeClassifier(max_depth=None, min_samples_split=max(m//100, 6), min_samples_leaf=max(m//200,3))
#md = DecisionTreeClassifier(max_depth=None, min_samples_split=6, min_samples_leaf=5)
md.fit(Xtrain, ytrain)
acc = md.score(Xtest, ytest)
print("accuracy", acc)
print(md.feature_importances_.round(3).tolist())


###

def gini(*_):
    a,b = _ if(len(_)==2) else _[0] if(len(_)==1) else (None,None)
    total = sum((a,b))
    g = 1 - (a/total)**2 - (b/total)**2
    return(g)  #g = gini-impurity-measure

def Gini(*_):
    a,b,c,d = _ if(len(_)==4) else _[0] if(len(_)==1) else (None,)*4
    g1 = gini(a,b)
    g0 = gini(c,d)
    n1 = sum([a,b])
    n0 = sum([c,d])
    N = n1+n0
    G = (n1/N)*g1 + (n0/N)*g0
    return(G)
    


n = X.shape[1]
l = list()
for j in range(n):
    t = tuple(np.c_[X[:,j], y].astype("uint8").tolist())
    values = ([1,1],[1,0],[0,1],[0,0])
    counts = [t.count(v) for v in values]
    G = Gini(counts)
    print(j,G)
    l.append((j,G))

bestGini = min(l, key=lambda t:t[-1])
bestRootNode = min(l, key=lambda t:t[-1])[0]


