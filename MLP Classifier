
import numpy as np


def load_from_github(url):
    from urllib.request import urlopen
    from os import remove
    
    obj = urlopen(url)
    assert obj.getcode()==200,"unable to open"

    s = str(obj.read(), encoding="utf-8")
    NAME = "_temp.py"
    with open(NAME, mode='wt', encoding='utf-8') as fh: fh.write(s)
    module = __import__(NAME[:-3])
    remove(NAME)
    return module


#===========================================================


from sklearn.base import BaseEstimator, ClassifierMixin

class MLP(BaseEstimator, ClassifierMixin):
    def __init__(self, layers, activation=None, validation=None, verbose=None):
        self.layers = tuple(layers)
        self.activation = get_activation_function(activation)
        self.output_activation = None
        self.validation = validation
        self.verbose = verbose
        
    def fit(self, X,y):
        m,n = X.shape
        classes = sorted(set(y))
        K = len(classes)
        
        self.output_activation = get_activation_function("sigmoid" if n==2 else "softmax")
        
        shapes = (n,) + self.layers + (K,)
        shapes = [(m,n) for m,n in zip(shapes[1:],shapes[:-1])]
        print(shapes)
    
    def probabilities(self, X):
        pass
    
    def predict(self, X):
        self.linear
        pass
    



#===================================================================

def get_activation_function(name):
    d = dict()
    
    def register(func):
        nonlocal d
        d.setdefault(func.__name__.lower().strip(), func)
        return func
    
    #paste your activation functions here:
    @register
    def linear(input, derivative=False, initializer=False):
        return input
    
    @register
    def relu(input, derivative=False, initializer=False):
        return input
    
    @register
    def leakyrelu(input, derivative=False, initializer=False):
        return input
    
    @register
    def tanh(input, derivative=False, initializer=False):
        return input
    
    @register
    def sigmoid(input, derivative=False, initializer=False):
        return input
    
    @register
    def softmax(input, derivative=False, initializer=False):
        return input
    
    #provide for double names (of activation functions)
    t = (
    ['logistic','sigmoid'],
    ['leaky','leakyrelu','leaky-relu','leaky_relu'],
    ['tanh','tangent hyporbolic']
    )
    
    for i in range(len(t)):
        l = [s for s in d.keys() if s in t[i]]
        if l:
            t[i].remove(l[0])
            [d.__setitem__(k, d[l[0]]) for k in t[i]]
    
    #get the appropriate activation function from the dictionary
    if name is None:
        return d["sigmoid"]
    else:
        try: return d[str(name).lower().strip()]
        except KeyError:
            msg = "No such activation function: " + str(name)
            raise ValueError(msg)



####################################################################################




url = r"https://raw.githubusercontent.com/leztien/synthetic_datasets/master/make_data_for_ANN.py"
module = load_from_github(url)
X,y = module.make_data_for_ANN(m=1000, n=3, K=5, L=3, u=16,
                               balanced_classes=True, space_between_classes=True, seed=None)



md = MLP(layers=(64,32,16), activation="relu")
md.fit(X,y)


